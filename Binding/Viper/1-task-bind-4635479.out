=== RUN START Fri Nov 14 22:09:02 CET 2025 on vipa1252 ===
SLURM: JOB_ID=4635479 NODES=1 NTASKS=1 TPERNODE=1 CPUS_PER_TASK=24
[INFO] USE_SRUN_FOR_BC=0 (1 => B and C will use srun when >1 tasks allocated)

========== SLURM VERSION ==========
slurm 24.11.6
slurm 24.11.6
slurm 24.11.6

========== SLURM FULL JOB SETTINGS ==========
JobId=4635479 JobName=affinity12.sh
   UserId=fdupros(2008) GroupId=rzg(4133) MCS_label=N/A
   Priority=5582647 Nice=0 Account=rzg_apu QOS=a0008
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   DerivedExitCode=0:0
   RunTime=00:00:03 TimeLimit=00:15:00 TimeMin=N/A
   SubmitTime=2025-11-14T22:08:57 EligibleTime=2025-11-14T22:08:57
   AccrueTime=2025-11-14T22:08:57
   StartTime=2025-11-14T22:08:59 EndTime=2025-11-14T22:23:59 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-11-14T22:08:59 Scheduler=Main
   Partition=apu AllocNode:Sid=viper12:1577040
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=vipa1252
   BatchHost=vipa1252
   NumNodes=1 NumCPUs=96 NumTasks=1 CPUs/Task=24 ReqB:S:C:T=0:0:*:1
   ReqTRES=cpu=24,mem=220000M,node=1,billing=440,gres/gpu=2
   AllocTRES=cpu=96,mem=220000M,node=1,billing=512,gres/gpu=2
   Socks/Node=* NtasksPerN:B:S:C=1:0:*:1 CoreSpec=*
   JOB_GRES=gpu:2
     Nodes=vipa1252 CPU_IDs=0-95 Mem=220000 GRES=gpu:2(IDX:0-1)
   MinCPUsNode=24 MinMemoryNode=0 MinTmpDiskNode=0
   Features=apu DelayBoot=00:00:00
   Reservation=amd
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/viper/u2/fdupros/affinity12.sh
   WorkDir=/viper/u2/fdupros
   StdErr=/viper/u2/fdupros/bind-4635479.error
   StdIn=/dev/null
   StdOut=/viper/u2/fdupros/bind-4635479.out
   TresPerNode=gres/gpu:2
   TresPerTask=cpu=24
   

StepId=4635479.batch UserId=2008 StartTime=2025-11-14T22:08:59 TimeLimit=UNLIMITED
   State=RUNNING Partition=apu NodeList=vipa1252
   Nodes=1 CPUs=0 Tasks=1 Name=batch Network=(null)
   TRES=(null)
   ResvPorts=(null)
   CPUFreqReq=Default
   SrunHost:Pid=(null):0


========== SOFTWARE VERSIONS (HIP, ROCm) ==========


============================ ROCm System Management Interface ============================
============================== Version of System Component ===============================
Driver version: 6.14.14
==========================================================================================
======================================= PCI Bus ID =======================================
GPU[0]		: PCI Bus: 0000:01:00.0
GPU[1]		: PCI Bus: 0000:81:00.0
==========================================================================================
====================================== Product Info ======================================
GPU[0]		: Card Series: 		AMD Radeon Graphics
GPU[0]		: Card Model: 		0x74a0
GPU[0]		: Card Vendor: 		Advanced Micro Devices, Inc. [AMD/ATI]
GPU[0]		: Card SKU: 		N/A
GPU[0]		: Subsystem ID: 	0x74a0
GPU[0]		: Device Rev: 		0x00
GPU[0]		: Node ID: 		2
GPU[0]		: GUID: 		10326
GPU[0]		: GFX Version: 		gfx942
GPU[1]		: Card Series: 		AMD Radeon Graphics
GPU[1]		: Card Model: 		0x74a0
GPU[1]		: Card Vendor: 		Advanced Micro Devices, Inc. [AMD/ATI]
GPU[1]		: Card SKU: 		N/A
GPU[1]		: Subsystem ID: 	0x74a0
GPU[1]		: Device Rev: 		0x00
GPU[1]		: Node ID: 		3
GPU[1]		: GUID: 		34871
GPU[1]		: GFX Version: 		gfx942
==========================================================================================
================================== End of ROCm SMI Log ===================================
HIP version: 6.4.43484-123eb5128
AMD clang version 19.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.4.3 25224 d366fa84f3fdcbd4b10847ebd5db572ae12a34fb)
Target: x86_64-unknown-linux-gnu
Thread model: posix
InstalledDir: /viper/u2/system/soft/RHEL_9/packages/x86_64/rocm/6.4.3/lib/llvm/bin
Configuration file: /viper/u2/system/soft/RHEL_9/packages/x86_64/rocm/6.4.3/lib/llvm/bin/clang++.cfg
6.4.43484-123eb5128HIP version: 6.4.43484-123eb5128

==hipconfig
HIP_PATH           :/mpcdf/soft/RHEL_9/packages/x86_64/rocm/6.4.3
ROCM_PATH          :/mpcdf/soft/RHEL_9/packages/x86_64/rocm/6.4.3
HIP_COMPILER       :clang
HIP_PLATFORM       :amd
HIP_RUNTIME        :rocclr
CPP_CONFIG         : -D__HIP_PLATFORM_HCC__= -D__HIP_PLATFORM_AMD__= -I/mpcdf/soft/RHEL_9/packages/x86_64/rocm/6.4.3/include -I/include

==hip-clang
HIP_CLANG_PATH     :/mpcdf/soft/RHEL_9/packages/x86_64/rocm/6.4.3/lib/llvm/bin
AMD clang version 19.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.4.3 25224 d366fa84f3fdcbd4b10847ebd5db572ae12a34fb)
Target: x86_64-unknown-linux-gnu
Thread model: posix
InstalledDir: /viper/u2/system/soft/RHEL_9/packages/x86_64/rocm/6.4.3/lib/llvm/bin
Configuration file: /viper/u2/system/soft/RHEL_9/packages/x86_64/rocm/6.4.3/lib/llvm/bin/clang++.cfg
AMD LLVM version 19.0.0git
  Optimized build.
  Default target: x86_64-unknown-linux-gnu
  Host CPU: znver3

  Registered Targets:
    amdgcn - AMD GCN GPUs
    r600   - AMD GPUs HD2XXX-HD6XXX
    x86    - 32-bit X86: Pentium-Pro and above
    x86-64 - 64-bit X86: EM64T and AMD64
hip-clang-cxxflags :
 -O3
hip-clang-ldflags :
--driver-mode=g++ -O3 -Llib --hip-link

== Environment Variables
PATH =/mpcdf/soft/RHEL_9/packages/x86_64/rocm/6.4.3/share/rocprofiler-systems/bin:/mpcdf/soft/RHEL_9/packages/x86_64/rocm/6.4.3/bin:/u/fdupros/.pixi/bin:/u/fdupros/.local/bin:/u/fdupros/bin:/mpcdf/soft/RHEL_9/packages/x86_64/Modules/5.4.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/mpcdf/soft/RHEL_9/packages/x86_64/find-module/1.0/bin:/opt/rocm/bin
HIP_PATH=/mpcdf/soft/RHEL_9/packages/x86_64/rocm/6.4.3
LD_LIBRARY_PATH=/mpcdf/soft/RHEL_9/packages/x86_64/rocm/6.4.3/lib/llvm/lib:/mpcdf/soft/RHEL_9/packages/x86_64/rocm/6.4.3/lib

== Linux Kernel
Hostname      :
vipa1252
Linux vipa1252 5.14.0-427.94.1.el9_4.x86_64 #1 SMP PREEMPT_DYNAMIC Wed Oct 8 06:57:38 EDT 2025 x86_64 x86_64 x86_64 GNU/Linux
LSB Version:	n/a
Distributor ID:	RedHatEnterprise
Description:	Red Hat Enterprise Linux 9.4 (Plow)
Release:	9.4
Codename:	n/a


========== SYSTEM TOPOLOGY ==========
Architecture:                         x86_64
CPU(s):                               96
On-line CPU(s) list:                  0-95
Model name:                           AMD Instinct MI300A Accelerator
Thread(s) per core:                   2
Core(s) per socket:                   24
Socket(s):                            2
CPU(s) scaling MHz:                   100%
NUMA node(s):                         2
NUMA node0 CPU(s):                    0-23,48-71
NUMA node1 CPU(s):                    24-47,72-95
-- HWLOC (NUMA only) --
NUMANode L#0 (P#0 125GB)
NUMANode L#1 (P#1 126GB)
=====================================
[INFO] Compiling hello_jobstep (HIP + OpenMP, NUMA optional via dlopen)...
✓ Built: /tmp/hello_jobstep_gpu_4635479_1763706

========== EXPERIMENT A: hello_jobstep (HIP+OMP) OMP_NUM_THREADS=1 ==========
A: MPI 000 - OMP 000 - HWT 023 - NUMA 0 - Node vipa1252 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID 01

========== EXPERIMENT A: hello_jobstep (HIP+OMP) OMP_NUM_THREADS=2 ==========
A: MPI 000 - OMP 000 - HWT 023 - NUMA 0 - Node vipa1252 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID 01
A: MPI 000 - OMP 001 - HWT 015 - NUMA 0 - Node vipa1252 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID 01

========== EXPERIMENT A: hello_jobstep (HIP+OMP) OMP_NUM_THREADS=4 ==========
A: MPI 000 - OMP 000 - HWT 023 - NUMA 0 - Node vipa1252 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID 01
A: MPI 000 - OMP 001 - HWT 008 - NUMA 0 - Node vipa1252 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID 01
A: MPI 000 - OMP 002 - HWT 007 - NUMA 0 - Node vipa1252 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID 01
A: MPI 000 - OMP 003 - HWT 015 - NUMA 0 - Node vipa1252 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID 01
[INFO] Compiling pthreads_affinity (NUMA optional via dlopen)...
✓ Built: /tmp/pthreads_affinity_4635479_1763706

========== EXPERIMENT B: POSIX pthreads, THREADS=1 ==========
[B:Context] PID=1763945, THREADS=1
B: Node vipa1252 - PTHREAD 000 - HWT 081 - NUMA 1 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 

========== EXPERIMENT B: POSIX pthreads, THREADS=2 ==========
[B:Context] PID=1763947, THREADS=2
B: Node vipa1252 - PTHREAD 000 - HWT 024 - NUMA 1 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 
B: Node vipa1252 - PTHREAD 001 - HWT 042 - NUMA 1 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 

========== EXPERIMENT B: POSIX pthreads, THREADS=4 ==========
[B:Context] PID=1763950, THREADS=4
B: Node vipa1252 - PTHREAD 000 - HWT 081 - NUMA 1 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 
B: Node vipa1252 - PTHREAD 001 - HWT 026 - NUMA 1 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 
B: Node vipa1252 - PTHREAD 002 - HWT 052 - NUMA 0 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 
B: Node vipa1252 - PTHREAD 003 - HWT 015 - NUMA 0 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 
[INFO] Compiling omp_affinity (NUMA optional via dlopen)...
✓ Built: /tmp/omp_affinity_4635479_1763706

========== EXPERIMENT C: OpenMP, OMP_NUM_THREADS=1 ==========
C: Node vipa1252 - OMP 000 - HWT 081 - NUMA 1 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 

========== EXPERIMENT C: OpenMP, OMP_NUM_THREADS=2 ==========
C: Node vipa1252 - OMP 000 - HWT 043 - NUMA 1 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 
C: Node vipa1252 - OMP 001 - HWT 026 - NUMA 1 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 

========== EXPERIMENT C: OpenMP, OMP_NUM_THREADS=4 ==========
C: Node vipa1252 - OMP 000 - HWT 081 - NUMA 1 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 
C: Node vipa1252 - OMP 003 - HWT 015 - NUMA 0 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 
C: Node vipa1252 - OMP 001 - HWT 043 - NUMA 1 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 
C: Node vipa1252 - OMP 002 - HWT 026 - NUMA 1 - Affinity: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 
=== RUN END Fri Nov 14 22:09:07 CET 2025 ===
                                  
================================= 
Global information about the job: 
================================= 
  
Job owner: fdupros(2008)
Job name:  affinity12.sh
Node list: vipa1252
Job start: Fri Nov 14 22:08:59 CET 2025
Job end:   Fri Nov 14 22:09:09 CET 2025
Work dir:  /viper/u2/fdupros
Command:   /viper/u2/fdupros/affinity12.sh
  
  
  
==========================================================================================
Information on jobsteps (Note: MaxRSS/AveRSS is the maximum/average over all 
tasks of the per-task memory high-water marks; cf. "man sacct"): 
==========================================================================================
  
JobID            JobName NNodes NTasks  NCPUS       MaxRSS       AveRSS    Elapsed ExitCode
------------- ---------- ------ ------ ------ ------------ ------------ ---------- --------
4635479       affinity12      1            96                             00:00:10      0:0
4635479.0     hello_jobs      1      1     48        9.77M        9.77M   00:00:01      0:0
4635479.1     hello_jobs      1      1     48        9.78M        9.78M   00:00:02      0:0
4635479.2     hello_jobs      1      1     48        9.79M        9.79M   00:00:00      0:0
  
Maximum memory per node: 0.010260 GB (defined as MaxRSS*Ntasks/NNodes)
CPU utilization: 0.0 %
  
